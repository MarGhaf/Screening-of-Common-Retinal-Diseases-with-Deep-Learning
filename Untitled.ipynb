{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681e0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 Project: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0521faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08b3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23ac8b",
   "metadata": {},
   "source": [
    "As usual, we'll start by importing the necessary packages that we'll use in this lab.\n",
    "Pillow is a popular Python library that is used for image processing tasks, such as opening, manipulating, and saving different image file formats. Pillow provides a wide range of image processing functionalities, including image filtering, image enhancement, image transformations, and image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728c35e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\home\\anaconda3\\lib\\site-packages (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374e2bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\home\\anaconda3\\lib\\site-packages (2.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb9bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83755aff",
   "metadata": {},
   "source": [
    " keras.preprocessing.image and keras.utils.image_utils are modules in the Keras deep learning library that provide utility functions for working with image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b6b01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\home\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.30.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.20.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\home\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\home\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\home\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\home\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d6e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b775c16",
   "metadata": {},
   "source": [
    "The data that will be used for the model has been graciously prepared by University of California San Diego, Guangzhou Women and Children's Medical Center\n",
    "This dataset contains thousands of validated OCT and Chest X-Ray images  described and analyzed in \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\". The images are split into a training set and a testing set of independent patients. Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.\n",
    "We will use the processed CT images hosted by Casper for our neural networks. In order to access these data we will download it directly to our local hardware, and create a path to the images like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee1768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd49d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 108309 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define the image size and batch size\n",
    "IMG_SIZE = (512, 512)\n",
    "BATCH_SIZE = 62\n",
    "\n",
    "# Define the directory containing the class sub-directories\n",
    "data_dir = 'CellData/OCT/train'\n",
    "\n",
    "# Create an ImageDataGenerator object to preprocess the images\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate a dataframe with the images and their corresponding labels\n",
    "data = datagen.flow_from_directory(data_dir,\n",
    "                                    target_size=IMG_SIZE,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    class_mode='categorical')\n",
    "df_train = pd.DataFrame({'image': data.filenames, 'label': list(data.classes)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720d151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108309 entries, 0 to 108308\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   image   108309 non-null  object\n",
      " 1   label   108309 non-null  int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41572128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d95d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    51140\n",
       "0    37205\n",
       "1    11348\n",
       "2     8616\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3    0.472168\n",
       "0    0.343508\n",
       "1    0.104774\n",
       "2    0.079550\n",
       "Name: label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View value counts in dataset\n",
    "display(df_train['label'].value_counts())\n",
    "# View percentages of value counts in dataset\n",
    "display(df_train['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55bde295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "s = df_train.label.astype('category')\n",
    "\n",
    "S = s.cat.rename_categories (['CNV', 'DME', 'DRUSEN', 'NORMAL'])\n",
    "\n",
    "# Create a matplotlib figure and axis for countplot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Set style of seaborn plot\n",
    "sns.set_style('darkgrid')\n",
    "# Create the countplot for findings in dataframe\n",
    "ax = sns.countplot(data=df_train, x=S, palette='cool')   \n",
    "\n",
    "# Create title for countplot\n",
    "ax.set_title('Distribution of Images', fontsize=14)\n",
    "# Create X-axis\n",
    "ax.set_xlabel('Label', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "# Create names on the x axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939176e",
   "metadata": {},
   "source": [
    "Our dataset has a strong bias. Almost 47% of the data is Normal. This can be challenging for the network.\n",
    "I scaled the data by reducing the size of the data in each categories to the  8000 cases. Therefore, the model can be trained on the equal amount of cases. This ratio is more suitable for data entering the model.  from each cateories 8000 images is selected randomely and store in new train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ef071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define the image size and batch size\n",
    "IMG_SIZE = (512, 512)\n",
    "BATCH_SIZE = 62\n",
    "\n",
    "# Define the directory containing the class sub-directories\n",
    "data_dir = 'CellData/OCT/test'\n",
    "\n",
    "# Create an ImageDataGenerator object to preprocess the images\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate a dataframe with the images and their corresponding labels\n",
    "data = datagen.flow_from_directory(data_dir,\n",
    "                                    target_size=IMG_SIZE,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    class_mode='categorical')\n",
    "df_test = pd.DataFrame({'image': data.filenames, 'label': list(data.classes)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8ade255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    250\n",
       "1    250\n",
       "2    250\n",
       "3    250\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.25\n",
       "1    0.25\n",
       "2    0.25\n",
       "3    0.25\n",
       "Name: label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View value counts in dataset\n",
    "display(df_test['label'].value_counts())\n",
    "# View percentages of value counts in dataset\n",
    "display(df_test['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364c18f",
   "metadata": {},
   "source": [
    "The test dataset is balanc so it does not to be blanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca3b97",
   "metadata": {},
   "source": [
    "If you want to separate an equal number of files from different subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dc06c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "\n",
    "CNV_files = glob.glob(\"CellData/OCT/train/CNV/*.jpeg\")\n",
    "DME_files = glob.glob(\"CellData/OCT/train/DME/*.jpeg\")\n",
    "DRUSEN_files = glob.glob(\"CellData/OCT/train/DRUSEN/*.jpeg\")\n",
    "normal_files = glob.glob(\"CellData/OCT/train/NORMAL/*.jpeg\")\n",
    "\n",
    "num_files_to_select = 8000\n",
    "\n",
    "subdir_CNV_train = random.sample(CNV_files, num_files_to_select)\n",
    "subdir_DME_train = random.sample(DME_files, num_files_to_select)\n",
    "subdir_DRUSEN_train = random.sample(DRUSEN_files, num_files_to_select)\n",
    "subdir_Normal_train = random.sample(normal_files, num_files_to_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b8a7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the source directory and the destination directory\n",
    "source_dir = \"CellData/OCT/train\"\n",
    "dest_dir = \"CellData/OCT/subset_train\"\n",
    "\n",
    "# Create the destination directory if it does not exist\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Create the subdirectories in the destination directory\n",
    "subdirs = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(dest_dir, subdir)\n",
    "    if not os.path.exists(subdir_path):\n",
    "        os.makedirs(subdir_path)\n",
    "\n",
    "# Move the image files to the subdirectories\n",
    "for file in subdir_CNV_train:\n",
    "    shutil.move(file, os.path.join(dest_dir, 'CNV'))\n",
    "for file in subdir_DME_train:\n",
    "    shutil.move(file, os.path.join(dest_dir, 'DME'))\n",
    "for file in subdir_DRUSEN_train:\n",
    "    shutil.move(file, os.path.join(dest_dir, 'DRUSEN'))\n",
    "for file in subdir_Normal_train:\n",
    "    shutil.move(file, os.path.join(dest_dir, 'NORMAL'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ea3ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define the image size and batch size\n",
    "IMG_SIZE = (512, 512)\n",
    "BATCH_SIZE = 62\n",
    "\n",
    "# Define the directory containing the class sub-directories\n",
    "data_dir = 'CellData/OCT/subset_train'\n",
    "\n",
    "# Create an ImageDataGenerator object to preprocess the images\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate a dataframe with the images and their corresponding labels\n",
    "data = datagen.flow_from_directory(data_dir,\n",
    "                                    target_size=IMG_SIZE,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e3a8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for keras neural network implementation\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "742fae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics modules for checking and visualizing performance of models\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37d7530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and encode data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccd631e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.vis_utils import plot_model to display Neural Networks\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c6fdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model\n",
    "base_model = models.Sequential()\n",
    "\n",
    "# Add the appropriate layers\n",
    "base_model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(224, 224, 3)))\n",
    "base_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "base_model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "base_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "base_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "base_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "base_model.add(layers.Flatten())\n",
    "base_model.add(layers.Dense(64, activation='relu'))\n",
    "base_model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "base_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d51326d6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 108, 108, 32)      16416     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 43264)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                2768960   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,804,898\n",
      "Trainable params: 2,804,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Return a summary of our model\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f01d4778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Return image of model's architecture\n",
    "plot_model(base_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5bdf7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "train_data_dir = 'CellData/OCT/subset_train'\n",
    "test_data_dir = 'CellData/OCT/test'\n",
    "\n",
    "# Get all the data in the directory data/train (32000 images), and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=32000)\n",
    "\n",
    "# Get all the data in the directory data/test (1000 images), and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=1000 )\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0db422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ecf00",
   "metadata": {},
   "source": [
    "Note that we have four numpy arrays now: train_images, train_labels, test_images, and test_labels. We'll need to make some changes to the data in order to work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview an image\n",
    "array_to_img(train_images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "842221fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 64, 64, 3)\n",
      "(32000, 4)\n",
      "(1000, 64, 64, 3)\n",
      "(1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Preview the shape of both the images and labels for both the train and test sets (4 objects total)\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(test_images))\n",
    "print(np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "23aec504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train images and test images\n",
    "train_img_unrow = train_images.reshape(32, -1).T\n",
    "test_img_unrow = test_images.reshape(32 , -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b3b44c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288000, 32)\n",
      "(384000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Preview the shape of train_img_unrow and test_img_unrow\n",
    "print(np.shape(train_img_unrow))\n",
    "print(np.shape(test_img_unrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c153de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the class names and their corresponding integer indices\n",
    "train_generator.class_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "044262bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 512 into shape (786432,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16980/518475113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# y_train_onehot is now a 2D array with shape (num_samples, num_classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# each row is a one-hot encoded binary vector representing the class label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_labels_final\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0my_train_onehot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m786432\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 512 into shape (786432,4)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# assume y_train is a 1D array of class labels (e.g. [0, 1, 2, 0, 2, ...])\n",
    "num_classes = 4\n",
    "y_train_onehot = to_categorical(train_labels, num_classes)\n",
    "y_test_onehot = to_categorical(test_labels, num_classes)\n",
    "\n",
    "# y_train_onehot is now a 2D array with shape (num_samples, num_classes)\n",
    "# each row is a one-hot encoded binary vector representing the class label\n",
    "train_labels_final= y_trai_onehot.reshape(786432, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "11a1722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4, 4)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "976259db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_final = y_train_onehot.T\n",
    "test_labels_final = y_test_onehot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6662cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 32)\n",
      "(4, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels_final))\n",
    "print(np.shape(test_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4a8f6eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rescale the pixel values to be between 0 and 1\n",
    "train_img_final = train_img_unrow/255\n",
    "test_img_final = test_img_unrow/255\n",
    "\n",
    "type(test_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing our Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5f235c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f7f10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cca795",
   "metadata": {},
   "source": [
    "In order to pass this data into a neural network, we'll need to make sure that the data:\n",
    "- is purely numerical\n",
    "- contains no missing values\n",
    "- is normalized\n",
    "\n",
    "Let's begin by calling the DataFrame's .info() method to check the datatype of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c2ff05a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[761856062,62] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16980/3465134023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m62\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m761856062\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2099\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2100\u001b[1;33m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[0;32m   2101\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[761856062,62] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(62, activation='tanh', input_shape=(,)))\n",
    "model_1.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cca48729",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc92c82",
   "metadata": {},
   "source": [
    "Let's quickly inspect the shape of our model before training it and see how many training parameters we have. In the cell below, call the model's .summary() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7d7f73c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 62)                761856062 \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4)                 252       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 761,856,314\n",
      "Trainable params: 761,856,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "38ae7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 786432), found shape=(32, 64, 64, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16980/4196593315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 786432), found shape=(32, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "results_1 = model_1.fit(train_images,train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6203e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGNet model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ef1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2bacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
